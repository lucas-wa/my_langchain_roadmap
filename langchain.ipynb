{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "xIBuB7E-8wnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "OKjXSRN-9hsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "!cd llama.cpp && make"
      ],
      "metadata": {
        "id": "rrFG8e0M-GpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && python3 -m pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "VFs3rlVk_GT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && python3 convert.py models/7B/"
      ],
      "metadata": {
        "id": "svVMmGgj_yvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/llama.git"
      ],
      "metadata": {
        "id": "yua8Hge7o1XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama && pip install -e ."
      ],
      "metadata": {
        "id": "o-vMTVekpAci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Esse passo requisita\n",
        "!cd llama && ./download.sh"
      ],
      "metadata": {
        "id": "R8w76YCdpPJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv llama/llama-2-7b/ llama.cpp/models/\n",
        "!mv llama.cpp/models/llama-2-7b/ llama.cpp/models/7B\n",
        "!mv llama/tokenizer.model llama.cpp/models/\n",
        "!mv llama/tokenizer_checklist.chk llama.cpp/models/"
      ],
      "metadata": {
        "id": "_yZu4npAtIm_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi kaleido python-multipart uvicorn"
      ],
      "metadata": {
        "id": "Z-qdB77v_CgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && python3 -m pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "Co5rSQpZtvEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"llama.cpp/models/7B/params.json\", \"w\")\n",
        "content = \"\"\"{\"dim\": 4096, \"multiple_of\": 256, \"n_heads\": 32, \"n_layers\": 32, \"norm_eps\": 1e-05, \"vocab_size\": 32000}\"\"\"\n",
        "f.write(content)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "x7Uj6r0ADEgi"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && python3 convert.py models/7B/"
      ],
      "metadata": {
        "id": "W6rl7TWluE5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && ./quantize ./models/7B/ggml-model-f16.gguf ./models/7B/ggml-model-q4_0.gguf q4_0"
      ],
      "metadata": {
        "id": "w7Ti-XUluNxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && ./quantize ./models/7B/ggml-model-q4_0.gguf ./models/7B/ggml-model-q4_0-v2.gguf COPY"
      ],
      "metadata": {
        "id": "26OUm6z1A7_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "wC5T5KJB3enc"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "xJcOIhLbD7AP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
      ],
      "metadata": {
        "id": "cwW-SU2UD_WU"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LlamaCpp(\n",
        "    model_path=\"llama.cpp/models/7B/ggml-model-q4_0-v2.gguf\",\n",
        "    temperature=0.75,\n",
        "    max_tokens=2000,\n",
        "    top_p=1,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,\n",
        "    n_gpu_layers=40,\n",
        "    n_batch=512\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be06qNn7EDa_",
        "outputId": "0d6db117-9295-4c83-dd5a-aa3b9968513e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Question: A rap battle between Stephen Colbert and John Oliver\n",
        "\"\"\"\n",
        "llm(prompt)"
      ],
      "metadata": {
        "id": "ygCW2LwnE0d1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}